{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a function that, given a series of coin flips, will return the maximum likelihood estimate of the coin's bias. Accomplish this by trying all possible biases between 0 and 1 in steps of 0.05, and returning the bias that yields the highest log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) You might hope that there are better ways of solving problem 1 than systematic guessing -- and, indeed, there are. From calculus, we know that we may find the local optima (i.e., maxima or minima) of a function by differentiating it, and setting the result equal to 0. This applies to the likelihood function as well. Analytically derive the maximum likelihood solution for the coin's bias by differentiating the log-likelihood of a sequence of coin tosses with respect to $p = P(heads)$, and implement the result in a function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and known standard deviation $\\sigma = 10$, will return the maximum likelihood estimate for $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and known standard deviation $\\sigma = 10$, will return the maximum a posteriori estimate for $\\mu$. You may assume a prior belief for $\\mu$ that is $\\mu \\sim N(3, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and unknown standard deviation $\\sigma$, will return the maximum likelihood estimates for both $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooler stuff\n",
    "\n",
    "In this section of the exercises, you're going to write inference code for the models you coded yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Neural coding (Poisson-spiking neurons)\n",
    "\n",
    "In this example, we again consider a fictional neuronal subtype that fires according to the azimuthal (horizontal) angle of the eye in goldfish. This neuron has a firing rate function\n",
    "\n",
    "$$\n",
    "    f(x) = kx\n",
    "$$\n",
    "\n",
    "where $x \\in [0, 90]$ corresponds to the eye angle, $k$ is some positive constant, and $f(x)$ denotes the firing rate of the neuron at angle $x$ in Hz. It then emits a spike count $r$ according to a Poisson distribution with rate parameter $f(x)$. That is, the distribution of possible spike counts $r$ at location $x$ is given by:\n",
    "\n",
    "$$\n",
    "P(r | x) = Poisson(f(x))\n",
    "$$\n",
    "\n",
    "In this exercise, we will attempt to infer the value of $k$. You may either use your own generative code from the previous problem set to create a dataset, or download the dataset we've generated for you off of the course GitHub page. \n",
    "\n",
    "If you completed this section of the Day 1 notebook, copy and paste your log-likelihood function below. Otherwise, write a function that accepts a vector $r$ of observed spike counts, a vector $x$ of recorded eye angles, and a real number $k$. This function should return the log-likelihood of the data (i.e., the log-probability of observing spike count $r[i]$ at eye angle $x[i]$ for all $i$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement grid search to infer $k$. As noted in lecture, grid search is simply the process of dividing up the regions of \"plausible\" values of $k$ into a fine grid, systematically trying each possible value, and then returning the best one.\n",
    "\n",
    "To that end, write a function that loops over possible values of $k$ from $0$ to $30$ in small steps (say, $0.1$). Let's denote the current guess value $\\hat{k}$. For each $\\hat{k}$ between $0$ and $30$, compute the log-likelihood of the data with $k = \\hat{k}$. At the end of the loop, you should return the $\\hat{k}$ that had the highest log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your inferred $k$ value to plot $f(x)$ for our fictional neuron, and overlay a scatterplot of the true data. How good does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: Download the second dataset on the course GitHub, and run your fitting and plotting functions on that dataset. What do you notice? Can you think of a way to alter the generative model to account for what you see? Edit your code above and see if you can figure out a way to fit both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Psychometric curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider the behaviour of a mouse performing an evidence accumulation task. The mouse hears a stream of randomly-generated clicks on both the right and left sides of its head. After 60 seconds pass, the mouse must pull a lever corresponding to the side that had the most clicks. \n",
    "\n",
    "Today, we will perform inference on a model of this task. Data has been provided for you on the course GitHub page, though you may also choose to use your own generative code if you have it.\n",
    "\n",
    "The details of the model in question are in the Day 1 notebook. If you completed this section of the Day 1 notebook, copy and paste the log-likelihood function you wrote into the cell below. Otherwise, write a function evaluating the log-likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're going to be using scipy's `minimize` function, which finds the minimum of a function using *fancy* methods. How should you adjust the code you've written above to do this? Write a function that will call `minimize` to return estimates for both $k$ and $x_0$. For those who are new to programming in general or Python in particular, I encourage you to try and figure out how to use this function from the documentation (reading documentation is a skill worth developing, just on its own).\n",
    "\n",
    "If this proves annoying, here are some hints:\n",
    "* `minimize` accepts, as its first argument, a function. In Python, functions can be passed as parameters by just using their name. You can pass the relevant data to `minimize` with the `args` parameter -- this will autofill everything after the very first parameter in the function you pass to `minimize`.\n",
    "* You are looking for the *maximum* likelihood solution, but `minimize` finds the target function's *minimum*. Can you write your target function in a way that its minimum is the likelihood's maximum?\n",
    "\n",
    "If this is especially challenging, you may implement this inference using grid search over the two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your inferred parameters to plot $f(x)$ for our mouse, and overlay a scatterplot of the true data. How good does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mouse exhibit a lateral bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in lecture, optimization is about finding parameters that maximize or minimize some objective function. To really visualize this idea, plot the log-likelihood landscape as a 3D surface as a function of $k, x_0$. Where do your fitted parameters end up relative to this landscape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
